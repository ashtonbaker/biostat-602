\documentclass[titlepage]{article}

%\linespread{1.25}

\usepackage{../stats-homework}
%\usepackage{baskervald}
%\usepackage[T1]{fontenc}


\begin{document}
\title{BIOSTAT 602 Biostatistical Inference\\Homework 01}
\author{Ashton Baker}
\date{Thursday January 12, 2017}
\maketitle
\begin{enumerate}
\item A coin is twice as likely to turn up tails as heads. If the coin is tossed independently, what is the probability that the third head occurs on the 5th trial?

\textbf{Solution.} This implies that $\P{Heads} = 1/3$ and $\P{Tails} = 2/3$. The probability of 2 heads occuring in the first 4 trials is described by a binomial distribution:
\[\binom{4}{2}(1/3)^2(2/3)^2 = 8/27\]
Then, the probability of heads occuring on the 5th trial is $1/3$, so the probability is
\[(8/27)(1/3) = 8/81\]

\item Suppose $X$ and $Y$ are two independent variables with unit variance. Let $Z = aX + Y$, where $a > 0$. If $\cor{X, Z} = 1/3$, then obtain the value of $a$.

\textbf{Solution.} By the definition of correlation,
\[\cor{X, Z} = \frac{\cov{X, Z}}{\sigma_X \sigma_Z} = 1/3\]
We can begin by finding $\cov{X, Z}$. Note that because $X$ and $Y$ are independent, $\cov{X, Y} = 0$.
\[\begin{aligned}
\cov{X, Z} &= \ev{XZ} - \ev{X}\ev{Z} \\
           &= \ev{aX^2 + XY} - \ev{X}\ev{aX + Y} \\
           &= a\ev{X^2} + \ev{XY} - a\ev{X}\ev{X} + \ev{X}\ev{Y} \\
           &= a \var{X} + \cov{X, Y} \\
           &= a
\end{aligned}\]
Because $X$ and $Y$ have unit variance,
\[\begin{aligned}
\sigma_Z &= \sqrt{\var{Z}} \\
         &= \sqrt{\var{aX + Y}} \\
         &= \sqrt{a^2 \var{X} + \var{Y} + 2a\cov{X, Y}} \\
         &= \sqrt{a^2 + 1}
\end{aligned}\]
and $\sigma_X = 1$. So
\[\begin{aligned}
\frac{\cov{X, Z}}{\sigma_X \sigma_Z} &= 1/3 \\
\frac{a}{\sqrt{a^2 + 1}} &= 1/3 \\
a &= \sqrt{a^2 + 1} / 3 \\
9a^2 &= a^2 + 1 \\
a^2 &= 1/8 \\
a &= \sqrt{1/8}
\end{aligned}\]

\item Let $g(x)$, $x \geq 0$ be a valid pdf for a nonnegative random variable and define
\[ f(x, y) = \frac{ g(\sqrt{x^2 + y^2}) }{ 2\pi \sqrt{x^2 + y^2} }\]
for $x, y \in \R$
  \begin{enumerate}
  \item Show that $f(x, y)$ is a valid pdf.

  \textbf{Solution.} The function $f$ depends on $x$ and $y$ only in terms of  $r = \sqrt{x^2 + y^2}$. So $f(x, y) = f(r)$, and
  \[\begin{aligned}
  \int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) \; dx \; dy &= \int_0^\infty 2\pi r f(r)\; dr \\
  &= \int_0^\infty 2 \pi r \frac{g(r)}{2 \pi r} \; dr\\
  &= \int_0^\infty g(r) \; dr \\
  &= 1
  \end{aligned}\]
  So $f$ is a valid pdf.

  \item Suppose that the pair $(X, Y)$ has the pdf $f(x, y)$. What is $P(XY > 0)$?

  \textbf{Solution.} The region where $XY > 0$ is the union of quadrant 1 and quadrant 3. Due to the radial symmetry of $f$, $P(XY > 0) = 1/2$.
  \end{enumerate}

\item Given independent and identically distributed random samples $X_1, X_2, \ldots, X_n$, each with finite mean $\mu$ and finite variance $\sigma^2$, define
\[\begin{aligned}
\Xbar &= \frac{1}{n} \sum_{i = 1}^n X_i \\
S^2 &= \frac{1}{n-1} \sum_{i = 1}^n \left(X_i - \Xbar\right)^2 \\
W^2 &= \frac{1}{n} \sum_{i = 1}^n \left(X_i - \Xbar\right)^2
\end{aligned}\]
  \begin{enumerate}
  \item Show that $S^2 \xrightarrow{P} \sigma^2$

  \textbf{Solution.} Note that
  \[\begin{aligned}
  \frac{n - 1}{n}S^2 &= \frac{1}{n}\sum_{i=1}^n(X_i - \Xbar)^2 \\
                     &= \frac{1}{n}\sum_{i=1}^n\left[(X_i - \mu) + (\mu - \Xbar)\right]^2 \\
                     &= \frac{1}{n}\left[\sum_{i=1}^n(X_i - \mu)^2 + 2(\mu - \Xbar)\sum_{i=1}^n(X_i - \mu) + \sum_{i=1}^n(\Xbar - \mu)^2 \right] \\
                     &= \frac{1}{n}\left[\sum_{i=1}^n(X_i - \mu)^2 + 2n(\mu - \Xbar)(\Xbar - \mu) + n(\Xbar - \mu)^2 \right] \\
                     &= \frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2 - 2(\mu - \Xbar)^2 + (\Xbar - \mu)^2.
  \end{aligned}\]
  Note that $\ev{(X_i - \mu)^2} = \sigma^2$, and $\ev{\Xbar - \mu} = 0$, so by the Weak Law of Large Numbers, $\frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2 \convprob \sigma^2$ and $(X_i - \mu)^2 \convprob 0$. Therefore $(n - 1)/n S^2 \convprob \sigma^2$. However, $(n - 1)/n \rightarrow 1$, so $S^2 \convprob \sigma^2$.

  \item Derive the asymptotic distribution of $\frac{\sqrt{n}\left(\Xbar - \mu\right)}{\sqrt{S^2}}$

  \textbf{Solution.} Let
    \[\begin{aligned}
    A_n &= \frac{\Xbar_n - \mu}{\sqrt{\sigma^2/n}}\\
    B_n &= \frac{\sigma^2}{S^2}
    \end{aligned}\]
    By the Central Limit Theorem, $A_n \convdist \N{0, 1}$. We have proved that $S^2 \convdist \sigma^2$, and since $B_n$ applies a continuous transformation to $S^2$, we have $B_n \convdist 1$. Then, by Slutsky's theorem,
    \[\frac{\sqrt{n}\left(\Xbar - \mu\right)}{\sqrt{S^2}} = A_n B_n \convdist \N{0, 1}\]

  \item Use the Delta method to derive the asymptotic distribution of $\Xbar^2$ after you normalize it appropriately.

  \textbf{Solution.} By the Central Limit Theorem, $\sqrt{n}(\Xbar -
\mu) \convdist \N{0, \sigma^2}$. Then, define $g(x) = x^2$, so that $g^{(1)}(x) = 2x$ is continuous. If $\mu \neq 0$, then we can apply the Delta Method to obtain
\[\begin{aligned}
\sqrt{n}\left[g(\Xbar) - g(\mu) \right] &\convdist \N{0, \sigma^2 \left[g^{(1)}(\mu) \right]^2} \\
\sqrt{n}\left[\Xbar^2 - \mu^2 \right] &\convdist \N{0, 4\sigma^2\mu^2}
\end{aligned}\]
  \end{enumerate}

\item For two sets of random varibales $\{X_i\}$, $i = 1, \ldots, n$, and $\{Y_i\}$, $j = 1, \ldots, m$, show that
\[\cov{\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j} = \sum_{i=1}^n \sum_{j=1}^m a_i b_j \cov{X_i, Y_j}\]
where $a_i$ and $b_j$ are arbitrary constants.

\textbf{Solution.}
\[\begin{aligned}
\cov{\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j} &= \ev{\left(\sum_{i=1}^n a_i X_i - \ev{\sum_{i=1}^n a_i X_i}\right) \left( \sum_{j=1}^m b_j Y_j - \ev{\sum_{j=1}^m b_j Y_j}\right)} \\
&= \ev{\sum_{i=1}^n a_i \left(X_i - \ev{X_i}\right)  \sum_{j=1}^m b_j \left(Y_j - \ev{Y_j}\right)} \\
&= \ev{\sum_{i=1}^n \sum_{j=1}^m a_i b_j \left(X_i - \ev{X_i}\right) \left(Y_j - \ev{Y_j}\right)} \\
&= \sum_{i=1}^n \sum_{j=1}^m a_i b_j \ev{\left(X_i - \ev{X_i}\right) \left(Y_j - \ev{Y_j}\right)} \\
&= \sum_{i=1}^n \sum_{j=1}^m a_i b_j \cov{X_i, Y_j} \\
\end{aligned}\]

\item Suppose $N \dist \pois(\lambda)$. Given $N = n > 0$, $X_1, \ldots, X_N$ are iid and follow $\U[0, 1]$. We define $X_0 = 0$ when $N = 0$.
  \begin{enumerate}
  \item Given $N = n$, find the probability that $X_0, X_1, \ldots, X_N$ are all less that $t$, where $0 < t < 1$.

  \textbf{Solution.} Since each $X_i$ is uniformly distributed, $\P{X_i < t} = t$. If $N = n$, then because $X_1, \ldots, X_n$ are independent,
  \[\P{X_1, \ldots, X_n < t} = \P{X_1 < t}\P{X_2 < t}\cdots\P{X_n < t} = t^n\]

  \item Find the (unconditional) probability that $X_0, X_1, \ldots, X_N$ are all less than $t$, where $0 < t < 1$.

  \textbf{Solution.} In this case, we need to sum over all the possible values that $N$ could take, weighted by the probability of observing each particular value $n$. So
  \[\begin{aligned}
  \P{X_1, \ldots, X_N < t} &= \sum_{n = 0}^\infty \P{N = n} \P{X_1, \ldots, X_n < t} \\
  &= \sum_{n = 0}^\infty \frac{\lambda^n e^{-\lambda}}{n!}t^n\\
  &= e^{-\lambda(1 - t)}
  \end{aligned}\]

  \item Let $S_N = X_0 + X_1 + \cdots + X_N$. Compute $\ev{S_N}$.

  \textbf{Solution.}
  \[\begin{aligned}
  \ev{S_N} &= \sum_{n = 0}^{\infty} \P{N = n} \ev{X_1 + \cdots + X_n} \\
  &= \sum_{n=0}^\infty \left(\frac{\lambda^n e^{-
  \lambda}}{n!} \right) \left(\frac{n}{2}\right)\\
  &= \frac{\lambda}{2}
  \end{aligned}\]
  \end{enumerate}

\end{enumerate}

\end{document}
